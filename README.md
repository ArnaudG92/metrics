# √âvaluation d‚Äôun syst√®me RAG

<img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https://substack-post-media.s3.amazonaws.com/public/images/0005cb44-e213-42ff-9dae-312b49c0b191_2000x1190.jpeg" alt="Image repr√©sentant un RAG" width="400"/>

## Sommaire

- [Introduction](#introduction)
- [Objectifs](#objectifs)
- [Solutions mises en place](#solutions-mises-en-place)
  - [1. Cr√©ation d‚Äôun dataset d‚Äô√©valuation](#1--la-cr√©ation-dun-dataset-d√©valuation)
  - [2. Embeddings](#2--lembeddings)
  - [3. Chunking](#3--chunking)
  - [4. Retrieval](#4--retrieval)
  - [5. Re-ranking](#5--re-ranking)
  - [6. R√©ponses du LLM](#6--les-r√©ponses-du-llm)
- [Applications Python](#les-applications-en-python)
  - [Retrieval](#python-retrieval) 
  - [Sans LLM](#python-retrieval-sans-llm)
  - [Avec RAGAS](#ragas-evaluation)
- [Note sur l‚Äôutilisation](#note-sur-l‚Äôutilisation)


## Introduction

Ce d√©p√¥t regroupe diff√©rentes solutions pour impl√©menter et √©valuer un syst√®me RAG √† l‚Äôaide de plusieurs outils, dans le but de comparer leur performance et leur pertinence.

**Documents suppl√©mentaires cr√©√©s**

- [Carte mentale (√† finir)](https://www.canva.com/design/DAGpTs9hinY/zjeUoHolQ4k2BgugcSM48Q/view?utm_content=DAGpTs9hinY&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=hecde141282)

## Objectifs

Ce projet a pour objectif d‚Äô√©valuer en profondeur les diff√©rentes composantes d‚Äôun syst√®me RAG (Retrieval-Augmented Generation), en testant plusieurs m√©thodes √† chaque √©tape de la cha√Æne de traitement.

L‚Äô√©valuation porte sur :

- **La cr√©ation d‚Äôun dataset d‚Äô√©valuation** adapt√© aux besoins du projet.
- **La qualit√© des embeddings**, en comparant diff√©rentes m√©thodes de vectorisation des documents.
- **Les strat√©gies de chunking**, afin d‚Äôanalyser leur impact sur la pertinence des passages extraits.
- **Les techniques de retrieval**, en testant divers mod√®les de recherche d‚Äôinformation.
- **Les m√©thodes de re-ranking**, pour optimiser l‚Äôordre des documents r√©cup√©r√©s.
- **L‚Äô√©valuation finale des r√©ponses g√©n√©r√©es**, √† l‚Äôaide de m√©triques fournies par des outils comme RAGAS (factualit√©, compl√©tude, etc.).

L‚Äôobjectif global est de comparer ces diff√©rentes approches pour identifier les combinaisons les plus efficaces et pertinentes dans un cadre d‚Äôutilisation concret.

## Solutions mises en place

### 1- La cr√©ation d‚Äôun dataset d‚Äô√©valuation

La premi√®re √©tape pour √©valuer un RAG est la cr√©ation d'un dataset de question/r√©ponse. Pour cela on utilise un document fictif (pour les tests) d'un [club de foot](data/output/story.md) invent√©. On compare diff√©rentes m√©thodes possibles : 

- [RAGAS](ragas-dataset)

### 2- L'Embeddings

blaaaaa

### 3- Chunking

blaaaaa

### 4- Retrieval

```metrics_simple_retrieval.py```
- Regroupement des diff√©rentes m√©thodes dans la carte mentale

Le principal probl√®me est de savoir si un document **est pertinent ou ne l'est pas** pour nos √©valuations. √âtiqueter manuellement la pertinence des documents est trop co√ªteux. Pour pallier cela, on utilise un LLM juge, capable d‚Äôannoter chaque document avec un score de pertinence :

| Score | Signification         |
| ----- | --------------------- |
| 3     | Tr√®s pertinent        |
| 2     | Pertinent             |
| 1     | Moyennement pertinent |
| 0     | Pas pertinent         |

  [Exemple en python](#python-retrieval)

### 5- Re-ranking

```metrics_simple_retrieval.py``` (dans la deuxi√®me partie)
- [Exemple en python](#python-retrieval)

### 6- Les r√©ponses du LLM

Deux m√©thodes possibles : 

- **M√©trique sans utilisation de LLM**
- **M√©trique avec utilisation de LLM**

Pour les m√©triques qui n'utilisent pas de **LLM**, on retrouve la liste suivante (expliqu√© dans la carte mentale) : 
- Bleu score
- Rouge score
- BERTScore
- Meteor
- MoverScore

[Impl√©mentation en Python](#python-evaluation-reponse-sans-LLM)

Pour les m√©triques qui utilisent un **LLM**, on retrouve la liste suivante (expliqu√© dans la carte mentale) : 
- RAGAS -> de nombreuses m√©triques qui analysent les r√©ponses sur plusieurs points ([plus d'info](#ragas-evaluation))


## Les applications en Python

### RAGAS Dataset

Premier r√©sultats √©valu√©s par Chatgpt : 


| #  | Question                                                                 | Extrait de la r√©ponse                                                                                               | Note /15 | √âvaluation                                                                                          |
|----|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------------------------------------|
| 1  | What was the significance of the 2023 season for √âtoile de la Plaine?    | The 2023 season marked a meteoric rise... stadium becoming full... community impact                                 | 14.5     | Tr√®s bonne question et r√©ponse. L√©g√®re coupure √† la fin du texte, mais excellent alignement Q/R.     |
| 2  | What is the significance of √âtoile de la Plaine‚Äôs success in 2023?       | ...meteoric rise, talented young players, community engagement...                                                    | 14       | Bonne profondeur dans la r√©ponse, mais question tr√®s proche de la 1 ‚Äî risque de redondance.         |
| 3  | What is the role of √âtoile de la Plaine in the club‚Äôs success?           | ...combinaison de talents, coach, engagement communautaire...                                                        | 11       | Question mal formul√©e, un peu floue. La r√©ponse est bonne, mais la Q doit √™tre clarifi√©e.           |
| 4  | What was the significance of the √âtoile de la Plaine‚Äôs success in 2023?  | ...joueurs comme Thomas, Nadir, Hugo... style de jeu sous Coach Mireille Dupont...                                   | 13.5     | Bonne r√©ponse, mais formulation redondante avec Q1 et Q2. Et erreur grammaticale ("the √âtoile").    |
| 5  | What is the significance of the match against AS Montroc?                | ...4-3 apr√®s prolongation... Nadir‚Äôs hat trick, Hugo‚Äôs save... transformation locale...                              | 15       | Question sp√©cifique et bien cibl√©e. R√©ponse parfaite, riche et coh√©rente.                           |
| 6  | What is the role of the captain, Thomas Duvauchel, in the story?         | ...vision du jeu, distribution, calme...                                                                             | 13       | Bonne r√©ponse mais trop courte. Question claire mais "in the story" pourrait √™tre reformul√©.         |
| 7  | What is the significance of the team‚Äôs rise to prominence?               | ...jeunes joueurs, strat√©gie, communaut√©, passion des fans...                                                        | 14       | Bonne g√©n√©ralisation. La r√©ponse illustre bien les causes profondes de leur ascension.              |


**Premier probl√®me : contexte toujours le m√™me et questions qui se r√©p√®tent**


### Python Retrieval 

Le fichier python correspondant : ```metrics_for_LLM_response_reference.py```

On discerne deux types de m√©triques : 
- Celles qui prennent compte du **classemment**
- Celles qui ne prennent pas compte du **classemment**

Pour celle qui ne prennent pas en compte le classement (d√©but du programme) :
- Pr√©cision
- Rappel
- F1-score

**PS** : Utilisation du module **sklearn.metrics**

Donn√©es : 

``` python
y_pred = [1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0]
y_true = [1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1]
```
- 1: Doc pertinent
- 0: Doc non pertinent

Pour celle qui prennent en compte le classement (fin du programme) :
- MRR (Mean Reciprocal Rank)
- Gain cumulatif actualis√© normalis√©

Ces m√©triques √©valuent si les documents les plus pertinents sont bien en haut du classement.

Utilisation de ```ir_measures``` : ‚ö†Ô∏è n√©cessite un compilateur pour lancement

### Python √©valuation r√©ponse sans LLM

Le fichier python correspondant : ```metrics_simple_retrieval.py```

Dans ce fichier, √† partir d'une r√©ponse de r√©f√©rence et d'une question g√©ner√©e, on calcule les diff√©rentes m√©triques que l'on affiche dans un graphique √† barres.

Analyse des r√©sultats : 
- Bleu score ne comprends pas les synonyme (√† part si on veut des mots cl√©s, pas tr√®s utile)
- Rouge score int√©ressant : üõë R√©p√©tition de mot sans sens augmente le score ?
- BERTScore : rien √† re-dire

### RAGAS evaluation

Le fichier python correspondant :
- ```evaluate_llm_with_ragas_local.py``` (utilisation d'un LLM local)
- ```evaluate_llm_with_ragas_aws.py``` (utilisation d'un LLM sur AWS)

Pour modifier les donn√©es de traitement : ```./data/input```

Dans ce fichier, √† partir d'une r√©ponse de r√©f√©rence et d'une question g√©ner√©e, on calcule les diff√©rentes m√©triques que l'on affiche dans un graphique √† bar.

üõë Probl√®me de timeOut, le LLM prends trop de temps pour r√©pondre, on est donc oblig√© de l'empecher de travailler sur plusieurs m√©triques simultan√©ment 

``` Python 
my_config = RunConfig(
    max_workers=1,
)
```

**PS** : je n'ai trouv√© que cette solution 

Les r√©sultats des scores sont renseign√©s dans un fichier exel :  ```./data/ouput```

Il y a dans ce fichier l'√©valuation des m√©triques suivantes : 
- answer_correctness
- answer_relevancy
- faithfulness
- context_precision
- context_recall


üõë Le temps de r√©ponse sur l'Evaluation est **tr√®s long**

üõë Analyse des r√©sultats √† faire 

## A faire ... 

## üîê Note sur l‚Äôutilisation

Ce projet a √©t√© d√©velopp√© dans le cadre d‚Äôun stage.  
Toute r√©utilisation, modification ou diffusion du code est strictement interdite sans l‚Äôautorisation explicite de l‚Äôauteur.

Merci de respecter ce cadre.

