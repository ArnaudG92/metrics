# √âvaluation d‚Äôun syst√®me RAG

<img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https://substack-post-media.s3.amazonaws.com/public/images/0005cb44-e213-42ff-9dae-312b49c0b191_2000x1190.jpeg" alt="Image repr√©sentant un RAG" width="400"/>

## Sommaire

- [Introduction](#introduction)
- [Objectifs](#objectifs)
- [Solutions mises en place](#solutions-mises-en-place)
  - [1. Cr√©ation d‚Äôun dataset d‚Äô√©valuation](#1--la-cr√©ation-dun-dataset-d√©valuation)
  - [2. Embeddings](#2--lembeddings)
  - [3. Chunking](#3--chunking)
  - [4. Retrieval](#4--retrieval)
  - [5. Re-ranking](#5--re-ranking)
  - [6. R√©ponses du LLM](#6--les-r√©ponses-du-llm)
- [Applications Python](#les-applications-en-python)
  - [Retrieval](#python-retrieval) 
  - [Sans LLM](#python-retrieval-sans-llm)
  - [Avec RAGAS](#ragas-evaluation)


## Introduction

Ce d√©p√¥t regroupe diff√©rentes solutions pour impl√©menter et √©valuer un syst√®me RAG √† l‚Äôaide de plusieurs outils, dans le but de comparer leur performance et leur pertinence.

**Documents suppl√©mentaires cr√©√©s**

- [Carte mentale (√† finir)](https://www.canva.com/design/DAGpTs9hinY/zjeUoHolQ4k2BgugcSM48Q/view?utm_content=DAGpTs9hinY&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=hecde141282)

## Objectifs

Ce projet a pour objectif d‚Äô√©valuer en profondeur les diff√©rentes composantes d‚Äôun syst√®me RAG (Retrieval-Augmented Generation), en testant plusieurs m√©thodes √† chaque √©tape de la cha√Æne de traitement.

L‚Äô√©valuation porte sur :

- **La cr√©ation d‚Äôun dataset d‚Äô√©valuation** adapt√© aux besoins du projet.
- **La qualit√© des embeddings**, en comparant diff√©rentes m√©thodes de vectorisation des documents.
- **Les strat√©gies de chunking**, afin d‚Äôanalyser leur impact sur la pertinence des passages extraits.
- **Les techniques de retrieval**, en testant divers mod√®les de recherche d‚Äôinformation.
- **Les m√©thodes de re-ranking**, pour optimiser l‚Äôordre des documents r√©cup√©r√©s.
- **L‚Äô√©valuation finale des r√©ponses g√©n√©r√©es**, √† l‚Äôaide de m√©triques fournies par des outils comme RAGAS (factualit√©, compl√©tude, etc.).

L‚Äôobjectif global est de comparer ces diff√©rentes approches pour identifier les combinaisons les plus efficaces et pertinentes dans un cadre d‚Äôutilisation concret.

## Solutions mises en place

### 1- La cr√©ation d‚Äôun dataset d‚Äô√©valuation

blaaaaa

### 2- L'Embeddings

blaaaaa

### 3- Chunking

blaaaaa

### 4- Retrieval

```metrics_simple_retrieval.py```
- Regroupement des diff√©rentes m√©thodes dans la carte mentale

Le principal probl√®me est de savoir si un document **est pertinent ou ne l'est pas** pour nos √©valuations. √âtiqueter manuellement la pertinence des documents est trop co√ªteux. Pour pallier cela, on utilise un LLM juge, capable d‚Äôannoter chaque document avec un score de pertinence :

| Score | Signification         |
| ----- | --------------------- |
| 3     | Tr√®s pertinent        |
| 2     | Pertinent             |
| 1     | Moyennement pertinent |
| 0     | Pas pertinent         |

  [Exemple en python](#python-retrieval)

### 5- Re-ranking

```metrics_simple_retrieval.py``` (dans la deuxi√®me partie)
- [Exemple en python](#python-retrieval)

### 6- Les r√©ponses du LLM

Deux m√©thodes possibles : 

- **M√©trique sans utilisation de LLM**
- **M√©trique avec utilisation de LLM**

Pour les m√©triques qui n'utilisent pas de **LLM**, on retrouve la liste suivante (expliqu√© dans la carte mentale) : 
- Bleu score
- Rouge score
- BERTScore
- Meteor
- MoverScore

[Impl√©mentation en Python](#python-evaluation-reponse-sans-LLM)

Pour les m√©triques qui utilisent un **LLM**, on retrouve la liste suivante (expliqu√© dans la carte mentale) : 
- RAGAS -> de nombreuses m√©triques qui analysent les r√©ponses sur plusieurs points ([plus d'info](#ragas-evaluation))


## Les applications en Python

### Python Retrieval 

Le fichier python correspondant : ```metrics_for_LLM_response_reference.py```

On discerne deux types de m√©triques : 
- Celles qui prennent compte du **classemment**
- Celles qui ne prennent pas compte du **classemment**

Pour celle qui ne prennent pas en compte le classement (d√©but du programme) :
- Pr√©cision
- Rappel
- F1-score

**PS** : Utilisation du module **sklearn.metrics**

Donn√©es : 

``` python
y_pred = [1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0]
y_true = [1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1]
```
- 1: Doc pertinent
- 0: Doc non pertinent

Pour celle qui prennent en compte le classement (fin du programme) :
- MRR (Mean Reciprocal Rank)
- Gain cumulatif actualis√© normalis√©

Ces m√©triques √©valuent si les documents les plus pertinents sont bien en haut du classement.

Utilisation de ```ir_measures``` : ‚ö†Ô∏è n√©cessite un compilateur pour lancement

### Python √©valuation r√©ponse sans LLM

Le fichier python correspondant : ```metrics_simple_retrieval.py```

Dans ce fichier, √† partir d'une r√©ponse de r√©f√©rence et d'une question g√©ner√©e, on calcule les diff√©rentes m√©triques que l'on affiche dans un graphique √† barres.

Analyse des r√©sultats : 
- Bleu score ne comprends pas les synonyme (√† part si on veut des mots cl√©s, pas tr√®s utile)
- Rouge score int√©ressant : üõë R√©p√©tition de mot sans sens augmente le score ?
- BERTScore : rien √† re-dire

### RAGAS evaluation

Le fichier python correspondant :
- ```evaluate_llm_with_ragas_local.py``` (utilisation d'un LLM local)
- ```evaluate_llm_with_ragas_aws.py``` (utilisation d'un LLM sur AWS)

Pour modifier les donn√©es de traitement : ```./data/input```

Dans ce fichier, √† partir d'une r√©ponse de r√©f√©rence et d'une question g√©ner√©e, on calcule les diff√©rentes m√©triques que l'on affiche dans un graphique √† bar.

üõë Probl√®me de timeOut, le LLM prends trop de temps pour r√©pondre, on est donc oblig√© de l'empecher de travailler sur plusieurs m√©triques simultan√©ment 

``` Python 
my_config = RunConfig(
    max_workers=1,
)
```

**PS** : je n'ai trouv√© que cette solution 

Les r√©sultats des scores sont renseign√©s dans un fichier exel :  ```./data/ouput```

Il y a dans ce fichier l'√©valuation des m√©triques suivantes : 
- answer_correctness
- answer_relevancy
- faithfulness
- context_precision
- context_recall


üõë Le temps de r√©ponse sur l'Evaluation est **tr√®s long**

üõë Analyse des r√©sultats √† faire 

## A faire ... 


